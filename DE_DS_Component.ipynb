{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, explode, from_json , StructType, StringType, regexp_extract, collect_list, avg, log1p, exp, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[6]\") \\\n",
    "        .appName('Data Engineer Data Science Component')\\\n",
    "        .config('spark.ui.port', '4040')\\\n",
    "        .config(\"spark.driver.memory\", \"4g\")\\\n",
    "        .config('spark.executor.memory', '4g')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./Raw_data\"  # Replace with your directory path\n",
    "# base_dir = './test'\n",
    "\n",
    "df = spark.read.option(\"multiline\", True).option(\"recursiveFileLookup\", True).json(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Distribution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----+---------------+\n",
      "|subject_area                         |date|aggregationType|\n",
      "+-------------------------------------+----+---------------+\n",
      "|Molecular Biology                    |2021|Journal        |\n",
      "|Cell Biology                         |2021|Journal        |\n",
      "|Engineering (miscellaneous)          |2020|Journal        |\n",
      "|Physics and Astronomy (miscellaneous)|2020|Journal        |\n",
      "|Engineering (miscellaneous)          |2020|Journal        |\n",
      "|Physics and Astronomy (miscellaneous)|2020|Journal        |\n",
      "|Engineering (miscellaneous)          |2020|Journal        |\n",
      "|Physics and Astronomy (miscellaneous)|2020|Journal        |\n",
      "|Engineering (miscellaneous)          |2019|Journal        |\n",
      "|Physics and Astronomy (miscellaneous)|2019|Journal        |\n",
      "+-------------------------------------+----+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Topic Distribution Over Time\n",
    "\n",
    "Topic_Distribution_Columns = [\n",
    "    col(\"abstracts-retrieval-response.subject-areas.subject-area\").alias('subject_area'), #ดูจำนวน subject แต่ละอัน ในแต่ละปี\n",
    "    col(\"abstracts-retrieval-response.coredata.prism:coverDate\").alias('date'), # ดูปี\n",
    "    col(\"abstracts-retrieval-response.coredata.prism:aggregationType\").alias('aggregationType') #  เผื่อดูแยกแต่ละประเภทการตีพิมพ์\n",
    "]\n",
    "\n",
    "Topic_Distribution_df = df.select(*Topic_Distribution_Columns)\n",
    "\n",
    "Topic_Distribution_df_exploded = Topic_Distribution_df.withColumn('subject_area',explode(col('subject_area').getItem('$')))\n",
    "\n",
    "Topic_Distribution_df_year = Topic_Distribution_df_exploded.withColumn('date',regexp_extract(col('date'),r'(\\d{4})',1))\n",
    "\n",
    "Topic_Distribution_df_year.show(10,False)\n",
    "\n",
    "# Topic_Distribution_df_exploded.groupBy('subject_area').count().orderBy('count', ascending=False).show(10, False)\n",
    "\n",
    "# Topic_Distribution_df_year.filter(col('subject_area').isNull()).show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------+-----+\n",
      "|date|subject_area             |count|\n",
      "+----+-------------------------+-----+\n",
      "|2020|Psychiatric Mental Health|1    |\n",
      "|2019|Psychiatric Mental Health|1    |\n",
      "|2018|Psychiatric Mental Health|0    |\n",
      "|2022|Psychiatric Mental Health|1    |\n",
      "|2023|Psychiatric Mental Health|1    |\n",
      "|2021|Psychiatric Mental Health|2    |\n",
      "|2020|Numerical Analysis       |1    |\n",
      "|2019|Numerical Analysis       |1    |\n",
      "|2018|Numerical Analysis       |4    |\n",
      "|2022|Numerical Analysis       |7    |\n",
      "+----+-------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Topic Distribution Over Time\n",
    "\n",
    "Topic_Distribution_Columns = [\n",
    "    col(\"abstracts-retrieval-response.subject-areas.subject-area\").alias('subject_area'), #ดูจำนวน subject แต่ละอัน ในแต่ละปี\n",
    "    col(\"abstracts-retrieval-response.coredata.prism:coverDate\").alias('date'), # ดูปี\n",
    "    col(\"abstracts-retrieval-response.coredata.prism:aggregationType\").alias('aggregationType') #  เผื่อดูแยกแต่ละประเภทการตีพิมพ์\n",
    "]\n",
    "\n",
    "Topic_Distribution_df = df.select(*Topic_Distribution_Columns)\n",
    "\n",
    "\n",
    "#Topic_Distribution_df.select('subject_area').show(10, False)\n",
    "Topic_Distribution_df_exploded = Topic_Distribution_df.withColumn('subject_area',explode(col('subject_area').getItem('$')))\n",
    "\n",
    "Topic_Distribution_df_year = Topic_Distribution_df_exploded.withColumn('date',regexp_extract(col('date'),r'(\\d{4})',1))\n",
    "\n",
    "# Final_Topic_Distribution_df = Topic_Distribution_df_year.with\n",
    "# Final_Topic_Distribution_df.show(10, False)\n",
    "#Topic_Distribution_df_year.show(10, False)\n",
    "Topic_Distribution_Grouped = Topic_Distribution_df_year.groupBy('subject_area','date').count().orderBy('count', ascending=True)\n",
    "\n",
    "# Final_Topic_Distribution_df.show().orderBy('publication_count', ascending=False).show(10, False)\n",
    "# Topic_Distribution_df_exploded.groupBy('subject_area').count().orderBy('count', ascending=False).show(10, False)\n",
    "\n",
    "distinct_dates = Topic_Distribution_Grouped.select(\"date\").distinct()#.show()\n",
    "distinct_subject_areas = Topic_Distribution_Grouped.select(\"subject_area\").distinct()#.show()\n",
    "\n",
    "Combined_Dates_subject_areas = distinct_dates.crossJoin(distinct_subject_areas)#.show()\n",
    "Topic_Distribution_df_Result = Combined_Dates_subject_areas.join(Topic_Distribution_Grouped, [\"date\", \"subject_area\"], \"left\") \\\n",
    "    .fillna(0)\n",
    "Topic_Distribution_df_Result.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete! Check the output folder.\n"
     ]
    }
   ],
   "source": [
    "output_file = \"./output/Topic_Distribution_Data\"  # Replace with your desired output path\n",
    "# Save the extracted data to CSV\n",
    "try:\n",
    "    Topic_Distribution_df_year.coalesce(1).write.option(\"header\", True).mode('overwrite').csv(output_file)\n",
    "    print(\"Data extraction complete! Check the output folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./output/Topic_Distribution_Count_Subject_Area_Data\"  # Replace with your desired output path\n",
    "# Save the extracted data to CSV\n",
    "try:\n",
    "    Topic_Distribution_df_Result.coalesce(1).write.option(\"header\", True).mode('overwrite').csv(output_file)\n",
    "    print(\"Data extraction complete! Check the output folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstracts-retrieval-response.item.bibrecord.item-info.itemidlist.itemid //เอา SGR เป็นหลัก (ใช้ดู reference)\n",
    "#abstracts-retrieval-response.item.bibrecord.tail.bibliography.reference[0].ref-info.refd-itemidlist.itemid //เอา SGR เป็นหลัก (ใช้ดู reference)\n",
    "Citation_Network_Columns = [\n",
    "    col(\"abstracts-retrieval-response.coredata.dc:title\").alias('title'),\n",
    "    col(\"abstracts-retrieval-response.item.bibrecord.item-info.itemidlist.itemid\").alias('SGR_id'), #ดูจำนวน subject แต่ละอัน ในแต่ละปี\n",
    "]\n",
    "Citation_Network_df = df.select(*Citation_Network_Columns)\n",
    "\n",
    "Citation_Network_df_exploded = Citation_Network_df.withColumn('SGR_id',explode(col('SGR_id')))\n",
    "\n",
    "filtered_df = Citation_Network_df_exploded.filter(col(\"SGR_id\").getItem('@idtype') == (\"SGR\"))\n",
    "\n",
    "Title_SGR_ID_df = filtered_df.withColumn('SGR_id', col('SGR_id').getItem('$'))\n",
    "\n",
    "# Title_SGR_ID_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstracts-retrieval-response.item.bibrecord.tail.bibliography.reference[0].ref-info.refd-itemidlist.itemid //เอา SGR เป็นหลัก (ใช้ดู reference)\n",
    "Reference_Column = [\n",
    "    col(\"abstracts-retrieval-response.coredata.dc:title\").alias('title'),\n",
    "    col(\"abstracts-retrieval-response.item.bibrecord.tail.bibliography.reference\").alias('Ref'),\n",
    "]\n",
    "\n",
    "Reference_df = df.select(*Reference_Column)\n",
    "\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"ref-info\", StructType([\n",
    "        StructField(\"refd-itemidlist\", StructType([\n",
    "            StructField(\"itemid\", ArrayType(StructType([\n",
    "                StructField(\"$\", StringType(), True),\n",
    "                StructField(\"@idtype\", StringType(), True)\n",
    "            ])), True)\n",
    "        ]), True),\n",
    "    ]), True),\n",
    "    StructField(\"ref-fulltext\", StringType(), True),\n",
    "    StructField(\"@id\", StringType(), True),\n",
    "]))\n",
    "\n",
    "Reference_df_json = Reference_df.withColumn('Ref', from_json(col('Ref'), schema))\n",
    "\n",
    "Reference_df_exploded = Reference_df_json.withColumn('Ref',explode(col('Ref')))\n",
    "\n",
    "Reference_df_id = Reference_df_exploded.withColumn('Ref', col('Ref').getItem('ref-info').getItem('refd-itemidlist').getItem('itemid'))\n",
    "\n",
    "\n",
    "# schema = ArrayType(StructType([\n",
    "#     StructField(\"$\", StringType(), True),\n",
    "#     StructField(\"@idtype\", StringType(), True)\n",
    "# ]))\n",
    "# Reference_df_id_json = Reference_df_id.withColumn('Ref', from_json(col('Ref'), schema))\n",
    "\n",
    "Reference_df_id_exploded = Reference_df_id.withColumn('Ref',explode(col('Ref')))\n",
    "\n",
    "filtered_df_SGR = Reference_df_id_exploded.filter(col(\"Ref\").getItem('@idtype') == (\"SGR\"))\n",
    "\n",
    "Reference_df_SGR = filtered_df_SGR.withColumn('Ref', col('Ref').getItem('$'))\n",
    "\n",
    "Connection_df = Reference_df_SGR.join(Title_SGR_ID_df, Reference_df_SGR.title == Title_SGR_ID_df.title, 'left').drop(Title_SGR_ID_df.title).select('title','SGR_id','Ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete! Check the output folder.\n"
     ]
    }
   ],
   "source": [
    "output_file_1 = \"./output/Citation_network_title_id\"  \n",
    "output_file_2 = \"./output/Citation_network_ref\"  \n",
    "# Save the extracted data to CSV\n",
    "try:\n",
    "    Title_SGR_ID_df.coalesce(1).write.option(\"header\", True).mode('overwrite').csv(output_file_1)\n",
    "    Connection_df.coalesce(1).write.option(\"header\", True).mode('overwrite').csv(output_file_2)\n",
    "    print(\"Data extraction complete! Check the output folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citedby count Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Citedby_count_Column = [\n",
    "    # col(\"abstracts-retrieval-response.coredata.dc:identifier\").alias('identifier'),\n",
    "    col(\"abstracts-retrieval-response.coredata.prism:publicationName\").alias('publication_name'),\n",
    "    # col(\"abstracts-retrieval-response.coredata.prism:aggregationType\").alias('aggregationType'),\n",
    "    col(\"abstracts-retrieval-response.coredata.citedby-count\").alias('citedby_count'),\n",
    "    #เพิ่มเติม เป็นตัวแปรต้นสำหรับ predict\n",
    "    col(\"abstracts-retrieval-response.subject-areas.subject-area\").alias('subject_area'),\n",
    "    col(\"abstracts-retrieval-response.affiliation\").alias('affiliation'),\n",
    "    \n",
    "\n",
    "]\n",
    "Citedby_count_df = df.select(*Citedby_count_Column)\n",
    "\n",
    "Citedby_count_df = Citedby_count_df.withColumn(\n",
    "    'citedby_count', col('citedby_count').cast('int')\n",
    ")\n",
    "\n",
    "# เก็บไว้ก่อน\n",
    "# Journey_Count_By_Publication_Name = Journal_Impact_df.groupBy('publication_name').agg(\n",
    "#     sum('citedby_count').alias('total_citedby_count'),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "#     count('citedby_count').alias('number_of_articles')\n",
    "#     ).orderBy('total_citedby_count', ascending=False)\n",
    "# Journey_Count_By_Publication_Name.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start encoding subject_area\n",
    "Citedby_count_df_exploded = Citedby_count_df.withColumn('subject_area',explode(col('subject_area').getItem('$')))\n",
    "\n",
    "\n",
    "Citedby_count_df_grouped = Citedby_count_df_exploded.groupBy('publication_name').agg(\n",
    "    collect_list(\"subject_area\").alias(\"subject_area_list\"),\n",
    ")\n",
    "\n",
    "cv_subject = CountVectorizer(inputCol=\"subject_area_list\", outputCol=\"subject_area_vector\", binary=True)\n",
    "\n",
    "\n",
    "encode_subject_df = cv_subject.fit(Citedby_count_df_grouped).transform(Citedby_count_df_grouped).drop('subject_area_list').join(Citedby_count_df, 'publication_name', 'left').drop('subject_area')\n",
    "\n",
    "#start encoding affiliation\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"affiliation-city\", StringType(), True),\n",
    "    StructField(\"@id\", StringType(), True),\n",
    "    StructField(\"affilname\", StringType(), True),\n",
    "    StructField(\"affiliation-country\", StringType(), True),\n",
    "    StructField(\"@href\", StringType(), True),\n",
    "]))\n",
    "encode_subject_df_json = encode_subject_df.withColumn('affiliation', from_json(col('affiliation'), schema))\n",
    "\n",
    "encode_subject_df_json_exploded = encode_subject_df_json.withColumn('affiliation',explode(col('affiliation').getItem('affilname')))\n",
    "\n",
    "encode_subject_df_json_grouped = encode_subject_df_json_exploded.groupBy('publication_name').agg(\n",
    "    collect_list(\"affiliation\").alias(\"affiliation_list\"),\n",
    ")\n",
    "\n",
    "cv_affiliation = CountVectorizer(inputCol=\"affiliation_list\", outputCol=\"affiliation_vector\", binary=True)\n",
    "\n",
    "encode_affiliation_df = cv_affiliation.fit(encode_subject_df_json_grouped).transform(encode_subject_df_json_grouped).drop('affiliation_list').join(encode_subject_df_json, 'publication_name', 'left').drop('affiliation')\n",
    "\n",
    "\n",
    "#start encoding publication_name\n",
    "\n",
    "indexer_publication = StringIndexer(inputCol=\"publication_name\", outputCol=\"publication_name_index\")\n",
    "encoder_publication = OneHotEncoder(inputCol=\"publication_name_index\", outputCol=\"publication_name_vector\")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer_publication , encoder_publication])\n",
    "\n",
    "encode_df = pipeline.fit(encode_affiliation_df).transform(encode_affiliation_df).drop('aggregationType' , 'aggregationType_index', 'publication_name' , 'publication_name_index', 'identifier', 'identifier_index')\n",
    "\n",
    "# encode_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillna\n",
    "filled_df = encode_df.fillna({'citedby_count':encode_df.agg(avg('citedby_count').alias('avg')).first()['avg']})\n",
    "\n",
    "#drop outlier\n",
    "percentile = filled_df.approxQuantile('citedby_count', [0.25, 0.75], 0)\n",
    "Q1 = percentile[0]\n",
    "Q3 = percentile[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "drop_outlier_df = filled_df.filter((col('citedby_count') >= lower_bound) & (col('citedby_count') <= upper_bound))\n",
    "\n",
    "#catergorize 0 citation\n",
    "with_is_zero_df = drop_outlier_df.withColumn('is_zero_citedby_count', expr(\"CASE WHEN citedby_count = 0 THEN 1 ELSE 0 END\"))\n",
    "\n",
    "is_zero_indexer = StringIndexer(inputCol=\"is_zero_citedby_count\", outputCol=\"is_zero_citedby_count_index\")\n",
    "is_zero_encoder = OneHotEncoder(inputCol=\"is_zero_citedby_count_index\", outputCol=\"is_zero_citedby_count_vector\")\n",
    "\n",
    "pipeline = Pipeline(stages=[is_zero_indexer , is_zero_encoder])\n",
    "\n",
    "final_df = pipeline.fit(with_is_zero_df).transform(with_is_zero_df).drop('is_zero_citedby_count' , 'is_zero_citedby_count_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For tuning hyperparameter\n",
    "# Columns = [\"numTrees\", \"maxDepth\", \"subsamplingRate\", \"minInstancesPerNode\", \"featureSubsetStrategy\", \"rmse\", \"mae\", \"r2\"]\n",
    "\n",
    "# rf_param_df = pd.DataFrame(columns=Columns)\n",
    "\n",
    "# rf_param_df = pd.read_csv('./output/RF_param.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['affiliation_vector', 'subject_area_vector', 'publication_name_vector', 'is_zero_citedby_count_vector']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(final_df)\n",
    "data = data.select(\"features\", \"citedby_count\")\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42069)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gbt = GBTRegressor(\n",
    "#     featuresCol=\"features\", \n",
    "#     labelCol=\"citedby_count\", \n",
    "#     maxIter=10,\n",
    "#     maxDepth=15,\n",
    "#     stepSize=0.2,\n",
    "#     seed=42069\n",
    "#     )\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"citedby_count\", \n",
    "    numTrees=50, \n",
    "    maxDepth=10, \n",
    "    subsamplingRate=0.8,\n",
    "    minInstancesPerNode=5,\n",
    "    featureSubsetStrategy=\"auto\",\n",
    "    seed=42069\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbt_model = gbt.fit(train_data)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "\n",
    "# gbt_predictions = gbt_model.transform(test_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "model = rf_model\n",
    "predictions = rf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 4.431851466296313\n",
      "Mean Absolute Error (MAE): 3.0658565309429537\n",
      "R-Squared (R²): 0.30015175312732845\n"
     ]
    }
   ],
   "source": [
    "evaluator_rmse = RegressionEvaluator(labelCol=\"citedby_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"citedby_count\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"citedby_count\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
    "# rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "\n",
    "# gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
    "# rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "\n",
    "# gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
    "# rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "\n",
    "\n",
    "# print(f\"GBT RMSE: {gbt_rmse}  RF RMSE: {rf_rmse}\")\n",
    "# print(f\"GBT MAE: {gbt_mae}  RF MAE: {rf_mae}\")\n",
    "# print(f\"GBT R²: {gbt_r2}  RF R²: {rf_r2}\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-Squared (R²): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_row = [ rf.getNumTrees(), rf.getMaxDepth(), rf.getSubsamplingRate(), rf.getMinInstancesPerNode(), rf.getFeatureSubsetStrategy(), rmse, mae, r2]\n",
    "\n",
    "# rf_param_df.loc[len(rf_param_df)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_param_df.tail()\n",
    "\n",
    "# rf_param_df.drop(len(rf_param_df)-1, inplace=True)\n",
    "\n",
    "# rf_param_df.to_csv('./output/RF_param.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.orderBy(\"prediction\", ascending=False).show(10, False)\n",
    "\n",
    "# train_data.filter(col('citedby_count').isNull() | col('features').isNull()).show(10, False)\n",
    "\n",
    "# predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete! Check the output folder.\n"
     ]
    }
   ],
   "source": [
    "output_file = \"./output/Citedby_Prediction_Data.csv\"  # Replace with your desired output path\n",
    "# Save the extracted data to CSV\n",
    "try:\n",
    "    predictions.select('citedby_count', 'prediction').coalesce(1).write.option(\"header\", True).mode('overwrite').csv(output_file)\n",
    "    print(\"Data extraction complete! Check the output folder.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
